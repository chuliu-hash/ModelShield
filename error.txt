╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/xuzhen/ModelShield-master/Imitation_Model_training/train/finetune_imitation_model_my.py:24 │
│ 2 in <module>                                                                                    │
│                                                                                                  │
│   239 │   parser.add_argument("--local_rank", type=int)                                          │
│   240 │   # parser.add_argument("--action", type=str)                                            │
│   241 │   args = parser.parse_args()                                                             │
│ ❱ 242 │   fire.Fire(train)                                                                       │
│   243                                                                                            │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/fire/core.py:141 in     │
│ Fire                                                                                             │
│                                                                                                  │
│   138 │   context.update(caller_globals)                                                         │
│   139 │   context.update(caller_locals)                                                          │
│   140                                                                                            │
│ ❱ 141   component_trace = _Fire(component, args, parsed_flag_args, context, name)                │
│   142                                                                                            │
│   143   if component_trace.HasError():                                                           │
│   144 │   _DisplayError(component_trace)                                                         │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/fire/core.py:475 in     │
│ _Fire                                                                                            │
│                                                                                                  │
│   472 │     is_class = inspect.isclass(component)                                                │
│   473 │                                                                                          │
│   474 │     try:                                                                                 │
│ ❱ 475 │   │   component, remaining_args = _CallAndUpdateTrace(                                   │
│   476 │   │   │   component,                                                                     │
│   477 │   │   │   remaining_args,                                                                │
│   478 │   │   │   component_trace,                                                               │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/fire/core.py:691 in     │
│ _CallAndUpdateTrace                                                                              │
│                                                                                                  │
│   688 │   loop = asyncio.get_event_loop()                                                        │
│   689 │   component = loop.run_until_complete(fn(*varargs, **kwargs))                            │
│   690   else:                                                                                    │
│ ❱ 691 │   component = fn(*varargs, **kwargs)                                                     │
│   692                                                                                            │
│   693   if treatment == 'class':                                                                 │
│   694 │   action = trace.INSTANTIATED_CLASS                                                      │
│                                                                                                  │
│ /home/xuzhen/ModelShield-master/Imitation_Model_training/train/finetune_imitation_model_my.py:21 │
│ 8 in train                                                                                       │
│                                                                                                  │
│   215 │   if torch.__version__ >= "2" and sys.platform != "win32":                               │
│   216 │   │   model = torch.compile(model)                                                       │
│   217 │   print("trainer.train")                                                                 │
│ ❱ 218 │   trainer.train(resume_from_checkpoint = args.resume_from_checkpoint)                    │
│   219 │   logger.info("Save checkpointing...")                                                   │
│   220 │                                                                                          │
│   221 │   model.save_pretrained(output_dir)                                                      │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/transformers/trainer.py │
│ :1662 in train                                                                                   │
│                                                                                                  │
│   1659 │   │   inner_training_loop = find_executable_batch_size(                                 │
│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size  │
│   1661 │   │   )                                                                                 │
│ ❱ 1662 │   │   return inner_training_loop(                                                       │
│   1663 │   │   │   args=args,                                                                    │
│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,                                │
│   1665 │   │   │   trial=trial,                                                                  │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/transformers/trainer.py │
│ :1991 in _inner_training_loop                                                                    │
│                                                                                                  │
│   1988 │   │   │   │   │   │   │   xm.optimizer_step(self.optimizer)                             │
│   1989 │   │   │   │   │   elif self.do_grad_scaling:                                            │
│   1990 │   │   │   │   │   │   scale_before = self.scaler.get_scale()                            │
│ ❱ 1991 │   │   │   │   │   │   self.scaler.step(self.optimizer)                                  │
│   1992 │   │   │   │   │   │   self.scaler.update()                                              │
│   1993 │   │   │   │   │   │   scale_after = self.scaler.get_scale()                             │
│   1994 │   │   │   │   │   │   optimizer_was_run = scale_before <= scale_after                   │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/torch/cuda/amp/grad_sca │
│ ler.py:341 in step                                                                               │
│                                                                                                  │
│   338 │   │                                                                                      │
│   339 │   │   assert len(optimizer_state["found_inf_per_device"]) > 0, "No inf checks were rec   │
│   340 │   │                                                                                      │
│ ❱ 341 │   │   retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         │
│   342 │   │                                                                                      │
│   343 │   │   optimizer_state["stage"] = OptState.STEPPED                                        │
│   344                                                                                            │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/torch/cuda/amp/grad_sca │
│ ler.py:288 in _maybe_opt_step                                                                    │
│                                                                                                  │
│   285 │   def _maybe_opt_step(self, optimizer, optimizer_state, *args, **kwargs):                │
│   286 │   │   retval = None                                                                      │
│   287 │   │   if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):    │
│ ❱ 288 │   │   │   retval = optimizer.step(*args, **kwargs)                                       │
│   289 │   │   return retval                                                                      │
│   290 │                                                                                          │
│   291 │   def step(self, optimizer, *args, **kwargs):                                            │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/torch/optim/lr_schedule │
│ r.py:68 in wrapper                                                                               │
│                                                                                                  │
│     65 │   │   │   │   instance = instance_ref()                                                 │
│     66 │   │   │   │   instance._step_count += 1                                                 │
│     67 │   │   │   │   wrapped = func.__get__(instance, cls)                                     │
│ ❱   68 │   │   │   │   return wrapped(*args, **kwargs)                                           │
│     69 │   │   │                                                                                 │
│     70 │   │   │   # Note that the returned function here is no longer a bound method,           │
│     71 │   │   │   # so attributes like `__func__` and `__self__` no longer exist.               │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/torch/optim/optimizer.p │
│ y:140 in wrapper                                                                                 │
│                                                                                                  │
│   137 │   │   │   │   obj, *_ = args                                                             │
│   138 │   │   │   │   profile_name = "Optimizer.step#{}.step".format(obj.__class__.__name__)     │
│   139 │   │   │   │   with torch.autograd.profiler.record_function(profile_name):                │
│ ❱ 140 │   │   │   │   │   out = func(*args, **kwargs)                                            │
│   141 │   │   │   │   │   obj._optimizer_step_code()                                             │
│   142 │   │   │   │   │   return out                                                             │
│   143                                                                                            │
│                                                                                                  │
│ /usr/local/anaconda3/.conda/envs/modelshield/lib/python3.9/site-packages/transformers/optimizati │
│ on.py:436 in step                                                                                │
│                                                                                                  │
│   433 │   │   │   │   │   # Exponential moving average of gradient values                        │
│   434 │   │   │   │   │   state["exp_avg"] = torch.zeros_like(p.data)                            │
│   435 │   │   │   │   │   # Exponential moving average of squared gradient values                │
│ ❱ 436 │   │   │   │   │   state["exp_avg_sq"] = torch.zeros_like(p.data)                         │
│   437 │   │   │   │                                                                              │
│   438 │   │   │   │   exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]                │
│   439 │   │   │   │   beta1, beta2 = group["betas"]                                              │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.69 GiB total capacity; 21.82 GiB already allocated; 3.69 MiB free; 23.31 GiB reserved in total by PyTorch) 
If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|